{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1c2e0d",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Here is the R8 R52 dataset on kaggle: https://www.kaggle.com/weipengfei/ohr8r52\n",
    "<li>here is an example of using Glove embeddings with the R8 dataset: https://www.kaggle.com/dvircohen0/text-classification-with-word-vectors\n",
    "<li>Here is where to get the Glove embeddings 6b.300d: https://www.kaggle.com/thanakomsn/glove6b300dtxt\n",
    "<li>Here is the link to the paper I want to implement: Text Level Graph Neural Network for Text Classification (https://arxiv.org/pdf/1910.02356.pdf)\n",
    "<li>Here is a link to an implementation of the research paper with pytorch: https://github.com/Cynwell/Text-Level-GNN\n",
    "<li>Here is a compilation of other papers related to NLP and GNNs\n",
    "<li>Here is the linkedin of the guy who's code I am working off of: https://www.linkedin.com/in/cynwell/\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533c5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from time import time\n",
    "import argparse\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c164c",
   "metadata": {},
   "source": [
    "## GloveTokenizer\n",
    "Below, I'm using the GloVe tokenizer. it is introduced by Jeffrey Pennington, Richard Socher, Christopher D. Manning in 2014 in a paper titled <i> GloVe: Global Vectors for Word Representation </i> which can be found [here](https://nlp.stanford.edu/pubs/glove.pdf). GloVe is a count-based, unsupervised learning model that uses co-occurrence (how frequently two words appear together) statistics at a Global level to model the vector representations of words. Since the statistics are captured at a global level directly by the model, it is named the Global Vectors model.\n",
    "\n",
    "Here, I'm using the glove.6B.300d embeddings which is a .txt file where, on every line, the first entry is the word and the following 300 entries are the embedding. I define functions to encode strings, decode a list of integers, and and to get the embedding array from a list of integers. The file can be downloaded [here](https://www.kaggle.com/thanakomsn/glove6b300dtxt). I'm using the 300d GloVe embeddings, since that was what was used in the research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "411c8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.04656', '0.21318', '-0.0074364', '-0.45854', '-0.035639', '0.23643', '-0.28836', '0.21521', '-0.13486', '-1.6413', '-0.26091', '0.032434', '0.056621', '-0.043296', '-0.021672', '0.22476', '-0.075129', '-0.067018', '-0.14247', '0.038825', '-0.18951', '0.29977', '0.39305', '0.17887', '-0.17343', '-0.21178', '0.23617', '-0.063681', '-0.42318', '-0.11661', '0.093754', '0.17296', '-0.33073', '0.49112', '-0.68995', '-0.092462', '0.24742', '-0.17991', '0.097908', '0.083118', '0.15299', '-0.27276', '-0.038934', '0.54453', '0.53737', '0.29105', '-0.0073514', '0.04788', '-0.4076', '-0.026759', '0.17919', '0.010977', '-0.10963', '-0.26395', '0.07399', '0.26236', '-0.1508', '0.34623', '0.25758', '0.11971', '-0.037135', '-0.071593', '0.43898', '-0.040764', '0.016425', '-0.4464', '0.17197', '0.046246', '0.058639', '0.041499', '0.53948', '0.52495', '0.11361', '-0.048315', '-0.36385', '0.18704', '0.092761', '-0.11129', '-0.42085', '0.13992', '-0.39338', '-0.067945', '0.12188', '0.16707', '0.075169', '-0.015529', '-0.19499', '0.19638', '0.053194', '0.2517', '-0.34845', '-0.10638', '-0.34692', '-0.19024', '-0.2004', '0.12154', '-0.29208', '0.023353', '-0.11618', '-0.35768', '0.062304', '0.35884', '0.02906', '0.0073005', '0.0049482', '-0.15048', '-0.12313', '0.19337', '0.12173', '0.44503', '0.25147', '0.10781', '-0.17716', '0.038691', '0.08153', '0.14667', '0.063666', '0.061332', '-0.075569', '-0.37724', '0.01585', '-0.30342', '0.28374', '-0.042013', '-0.040715', '-0.15269', '0.07498', '0.15577', '0.10433', '0.31393', '0.19309', '0.19429', '0.15185', '-0.10192', '-0.018785', '0.20791', '0.13366', '0.19038', '-0.25558', '0.304', '-0.01896', '0.20147', '-0.4211', '-0.0075156', '-0.27977', '-0.19314', '0.046204', '0.19971', '-0.30207', '0.25735', '0.68107', '-0.19409', '0.23984', '0.22493', '0.65224', '-0.13561', '-0.17383', '-0.048209', '-0.1186', '0.0021588', '-0.019525', '0.11948', '0.19346', '-0.4082', '-0.082966', '0.16626', '-0.10601', '0.35861', '0.16922', '0.07259', '-0.24803', '-0.10024', '-0.52491', '-0.17745', '-0.36647', '0.2618', '-0.012077', '0.08319', '-0.21528', '0.41045', '0.29136', '0.30869', '0.078864', '0.32207', '-0.041023', '-0.1097', '-0.092041', '-0.12339', '-0.16416', '0.35382', '-0.082774', '0.33171', '-0.24738', '-0.048928', '0.15746', '0.18988', '-0.026642', '0.063315', '-0.010673', '0.34089', '1.4106', '0.13417', '0.28191', '-0.2594', '0.055267', '-0.052425', '-0.25789', '0.019127', '-0.022084', '0.32113', '0.068818', '0.51207', '0.16478', '-0.20194', '0.29232', '0.098575', '0.013145', '-0.10652', '0.1351', '-0.045332', '0.20697', '-0.48425', '-0.44706', '0.0033305', '0.0029264', '-0.10975', '-0.23325', '0.22442', '-0.10503', '0.12339', '0.10978', '0.048994', '-0.25157', '0.40319', '0.35318', '0.18651', '-0.023622', '-0.12734', '0.11475', '0.27359', '-0.21866', '0.015794', '0.81754', '-0.023792', '-0.85469', '-0.16203', '0.18076', '0.028014', '-0.1434', '0.0013139', '-0.091735', '-0.089704', '0.11105', '-0.16703', '0.068377', '-0.087388', '-0.039789', '0.014184', '0.21187', '0.28579', '-0.28797', '-0.058996', '-0.032436', '-0.0047009', '-0.17052', '-0.034741', '-0.11489', '0.075093', '0.099526', '0.048183', '-0.073775', '-0.41817', '0.0041268', '0.44414', '-0.16062', '0.14294', '-2.2628', '-0.027347', '0.81311', '0.77417', '-0.25639', '-0.11576', '-0.11982', '-0.21363', '0.028429', '0.27261', '0.031026', '0.096782', '0.0067769', '0.14082', '-0.013064', '-0.29686', '-0.079913', '0.195', '0.031549', '0.28506', '-0.087461', '0.0090611', '-0.20989', '0.053913']\n"
     ]
    }
   ],
   "source": [
    "filename = 'embeddings/glove.6B.300d.txt'\n",
    "unk='<unk>'\n",
    "pad='<pad>'\n",
    "stoi = dict() \n",
    "itos = dict() \n",
    "embedding_matrix = list() # contains the embeddings\n",
    "with open(filename, 'r', encoding='utf8') as f: # Read tokenizer file\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        # the first value in each line is the string, the rest of the values in the line is the embedding\n",
    "        stoi[values[0]] = i\n",
    "        itos[i] = values[0]\n",
    "        if i == 0:\n",
    "            print([v for v in values[1:]])\n",
    "        embedding_matrix.append([float(v) for v in values[1:]])\n",
    "if unk is not None: # Add unk token into the tokenizer\n",
    "    i += 1\n",
    "    stoi[unk] = i\n",
    "    itos[i] = unk\n",
    "    embedding_matrix.append(np.random.rand(len(embedding_matrix[0]))) # embeddings are random numbers between 0-1\n",
    "if pad is not None: # Add pad token into the tokenizer\n",
    "    i += 1\n",
    "    stoi[pad] = i\n",
    "    itos[i] = pad\n",
    "    embedding_matrix.append(np.zeros(len(embedding_matrix[0]))) # embeddings are all 0's\n",
    "embedding_matrix = np.array(embedding_matrix).astype(np.float32) # Convert from double to float for efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d10882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kronik\n",
      "rolonda\n",
      "zsombor\n",
      "sandberger\n",
      "<unk>\n",
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "# last 6 words in itos\n",
    "for i in range(i-5,i+1):\n",
    "    print(itos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875be00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      ",\n",
      ".\n",
      "of\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "# first 5 words in itos\n",
    "for this_i in range(0,5):\n",
    "    print(itos[this_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffa837",
   "metadata": {},
   "source": [
    "# Loading the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc42c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveTokenizer:\n",
    "    def __init__(self, filename, unk='<unk>', pad='<pad>'):\n",
    "        self.filename = filename\n",
    "        self.unk = unk # unknown token\n",
    "        self.pad = pad # pad token\n",
    "        self.stoi = dict() # string to int dictionary\n",
    "        self.itos = dict() # int to string dictionary\n",
    "        self.embedding_matrix = list() # contains the embeddings\n",
    "        with open(filename, 'r', encoding='utf8') as f: # Read tokenizer file\n",
    "            for i, line in enumerate(f):\n",
    "                values = line.split()\n",
    "                # the first value in each line is the string, the rest of the values in the line is the embedding\n",
    "                self.stoi[values[0]] = i\n",
    "                self.itos[i] = values[0]\n",
    "                self.embedding_matrix.append([float(v) for v in values[1:]])\n",
    "        if self.unk is not None: # Add unk token into the tokenizer\n",
    "            i += 1\n",
    "            self.stoi[self.unk] = i\n",
    "            self.itos[i] = self.unk\n",
    "            # embeddings are random numbers between 0-1\n",
    "            self.embedding_matrix.append(np.random.rand(len(self.embedding_matrix[0]))) \n",
    "        if self.pad is not None: # Add pad token into the tokenizer\n",
    "            i += 1\n",
    "            self.stoi[self.pad] = i\n",
    "            self.itos[i] = self.pad\n",
    "            # embeddings are all 0's\n",
    "            self.embedding_matrix.append(np.zeros(len(self.embedding_matrix[0]))) \n",
    "        # Convert from double to float for efficiency\n",
    "        self.embedding_matrix = np.array(self.embedding_matrix).astype(np.float32) \n",
    "\n",
    "    # to encode a string into numbers\n",
    "    def encode(self, sentence):\n",
    "        if type(sentence) == str:\n",
    "            # splits the string by \" \" and also, include punctuation since Glove has embeddings for punctuation\n",
    "            sentence = re.findall(r\"[\\w']+|[.,!?;]\", sentence)\n",
    "        elif len(sentence): # if it is not a string, but it is convertible to a list, then convert it\n",
    "            sentence = list(sentence)\n",
    "        else:\n",
    "            raise TypeError('sentence should be either a str or a list of str!')\n",
    "        encoded_sentence = list()\n",
    "        for word in sentence:\n",
    "            # encode each word using the string to int dictionary, if the word doesn't exist, use the unknown token\n",
    "            # converting the word to lower case since the stoi dictionary only has lower case words\n",
    "            # otherwise, all capitalized words would just be given the unknown token\n",
    "            encoded_sentence.append(self.stoi.get(word.lower(), self.stoi[self.unk])) \n",
    "        return encoded_sentence\n",
    "\n",
    "    # to decode numbers into a string\n",
    "    def decode(self, encoded_sentence):\n",
    "        try:\n",
    "            encoded_sentence = list(encoded_sentence)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise TypeError('encoded_sentence should be either a str or a data type that is convertible to list type!')\n",
    "        sentence = list()\n",
    "        for encoded_word in encoded_sentence:\n",
    "            sentence.append(self.itos[encoded_word])\n",
    "        return sentence\n",
    "\n",
    "    # takes an encoded sentence and returns the embeddings of shape (len(encoded_sentence), 300)\n",
    "    def embedding(self, encoded_sentence):\n",
    "        return self.embedding_matrix[np.array(encoded_sentence)]\n",
    "    \n",
    "# example:\n",
    "# tokenizer = GloveTokenizer('embeddings/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f093c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GloveTokenizer('embeddings/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587a495",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "Here, I am using the R8 dataset. This is a subset of the Reuters-21578 dataset, which is a collection of documents with news articles. The Reuters-21578 dataset is one of the most widely used data collections for text categorization research. It is collected from the Reuters financial newswire service in 1987. The Reuters-21578 dataset has 10,369 documents and a vocabulary of 29,930 words. It can be found [here](https://paperswithcode.com/dataset/reuters-21578). The R8 dataset, which is implemented in the research paper, has 5,485 training examples and 2189 test examples and 8 classes. When the R8 dataset was created, it removed all numbers from the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a309c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>earn</td>\n",
       "      <td>champion products ch approves stock split cham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acq</td>\n",
       "      <td>computer terminal systems cpml completes sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earn</td>\n",
       "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn</td>\n",
       "      <td>am international inc am nd qtr jan oper shr lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn</td>\n",
       "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0  earn  champion products ch approves stock split cham...\n",
       "1   acq  computer terminal systems cpml completes sale ...\n",
       "2  earn  cobanco inc cbco year net shr cts vs dlrs net ...\n",
       "3  earn  am international inc am nd qtr jan oper shr lo...\n",
       "4  earn  brown forman inc bfd th qtr net shr one dlr vs..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filename='r8-train-all-terms.txt'\n",
    "test_filename='r8-test-all-terms.txt'\n",
    "train_data = pd.read_csv(train_filename, sep='\\t', header=None)\n",
    "test_data = pd.read_csv(test_filename, sep='\\t', header=None)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bacac",
   "metadata": {},
   "source": [
    "# How TextLevelGNNDatasetClass.build_vocab() Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb4abee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'earn': [0, 0, 1, 0, 0, 0, 0, 0],\n",
       " 'acq': [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'trade': [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " 'ship': [0, 0, 0, 0, 0, 0, 1, 0],\n",
       " 'grain': [0, 0, 0, 1, 0, 0, 0, 0],\n",
       " 'crude': [0, 1, 0, 0, 0, 0, 0, 0],\n",
       " 'interest': [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " 'money-fx': [0, 0, 0, 0, 0, 1, 0, 0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the label dictionary is created, which is one-hot encoded\n",
    "label_dict = dict(zip(train_data[0].unique(), pd.get_dummies(train_data[0].unique()).values.tolist()))\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f00c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset is split into the training and test set\n",
    "train_validation_split=0.8\n",
    "train_dataset, validation_dataset = random_split(train_data.to_numpy(), [int(len(train_data) * train_validation_split), len(train_data) - int(len(train_data) * train_validation_split)])\n",
    "\n",
    "vocab_list = [re.findall(r\"[\\w']+|[.,!?;]\", sentence) for _, sentence in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0dedc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embedding matrix: (15394, 300)\n"
     ]
    }
   ],
   "source": [
    "# get the list of unique vocabulary\n",
    "vocab_list = [re.findall(r\"[\\w']+|[.,!?;]\", sentence) for _, sentence in train_dataset]\n",
    "unique_vocab = []\n",
    "\n",
    "stoi = {'<unk>': 0, '<pad>': 1} # Re-index\n",
    "itos = {0: '<unk>', 1: '<pad>'} # Re-index\n",
    "vocab_count = len(stoi)\n",
    "\n",
    "# vocab_list is a list of lists, so this creates unique_vocab which is a single list with all words\n",
    "for vocab in vocab_list:\n",
    "    unique_vocab.extend(vocab)\n",
    "# getting list of unique tokens, make sure they are all lower case since the tokenizer does not recognize upper case\n",
    "unique_vocab = [item.lower() for item in list(set(unique_vocab))]\n",
    "\n",
    "for vocab in unique_vocab:\n",
    "    # if the vocab word is recognized by the tokenizer, add the vocab to stoi, itos, and increment vocab_count by 1\n",
    "    if vocab in tokenizer.stoi.keys():\n",
    "        # only add the vocab to the stoi keys and increment the vocab_count if the vocab isn't already in stoi\n",
    "        if vocab not in stoi.keys():\n",
    "            stoi[vocab] = vocab_count\n",
    "            itos[vocab_count] = vocab\n",
    "            vocab_count += 1\n",
    "embedding_matrix = tokenizer.embedding(tokenizer.encode(list(stoi.keys())))   \n",
    "print(f'shape of embedding matrix: {embedding_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7aafc8",
   "metadata": {},
   "source": [
    "# How create_neighbor_set Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f36e52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neighbor_set(node_set, p=2):\n",
    "    if type(node_set[0]) != int:\n",
    "        raise ValueError('node_set should be a 1D list!')\n",
    "    if p < 0:\n",
    "        raise ValueError('p should be an integer >= 0!')\n",
    "    sequence_length = len(node_set)\n",
    "    neighbor_set = []\n",
    "    for i in range(sequence_length):\n",
    "        neighbor = []\n",
    "        for j in range(-p, p+1):\n",
    "            if 0 <= i + j < sequence_length:\n",
    "                neighbor.append(node_set[i+j])\n",
    "        neighbor_set.append(neighbor)\n",
    "    return neighbor_set\n",
    "# example:\n",
    "# single_nodeset = [8145, 15469, 446, 6223, 5523, 4492, 8145, 11493, 11578, 9559]\n",
    "# create_neighbor_set(single_nodeset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f3bd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "p = 2\n",
    "# gets the first MAX_LENGTH words from each sentence in train_dataset and converts them to ints\n",
    "node_sets = [[stoi.get(vocab, 0) for vocab in re.findall(r\"[\\w']+|[.,!?;]\", sentence)][:MAX_LENGTH] for _, sentence in train_dataset] # Only retrieve the first MAX_LENGTH words in each document\n",
    "neighbor_sets = [create_neighbor_set(node_set, p=p) for node_set in node_sets]\n",
    "labels = [label_dict[label] for label, _ in train_dataset]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebb618a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1043, 9855, 11196, 1675, 767, 3160, 4042, 1043, 9855, 6299]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da09ea04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1043, 9855, 11196],\n",
       " [1043, 9855, 11196, 1675],\n",
       " [1043, 9855, 11196, 1675, 767],\n",
       " [9855, 11196, 1675, 767, 3160],\n",
       " [11196, 1675, 767, 3160, 4042],\n",
       " [1675, 767, 3160, 4042, 1043],\n",
       " [767, 3160, 4042, 1043, 9855],\n",
       " [3160, 4042, 1043, 9855, 6299],\n",
       " [4042, 1043, 9855, 6299],\n",
       " [1043, 9855, 6299]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_sets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a8082",
   "metadata": {},
   "source": [
    "# How build_public_edge_mask Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07570982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbor_set: [[1043, 9855, 11196], [1043, 9855, 11196, 1675], [1043, 9855, 11196, 1675, 767], [9855, 11196, 1675, 767, 3160], [11196, 1675, 767, 3160, 4042], [1675, 767, 3160, 4042, 1043], [767, 3160, 4042, 1043, 9855], [3160, 4042, 1043, 9855, 6299], [4042, 1043, 9855, 6299], [1043, 9855, 6299]]\n",
      "neighbor: [1043, 9855, 11196]\n",
      "node_set: [1043, 9855, 11196, 1675, 767, 3160, 4042, 1043, 9855, 6299]\n",
      "to_node: 1043\n",
      "\n",
      "neighbor_set: [[1043, 9855, 11196], [1043, 9855, 11196, 1675], [1043, 9855, 11196, 1675, 767], [9855, 11196, 1675, 767, 3160], [11196, 1675, 767, 3160, 4042], [1675, 767, 3160, 4042, 1043], [767, 3160, 4042, 1043, 9855], [3160, 4042, 1043, 9855, 6299], [4042, 1043, 9855, 6299], [1043, 9855, 6299]]\n",
      "neighbor: [1043, 9855, 11196]\n",
      "node_set: [1043, 9855, 11196, 1675, 767, 3160, 4042, 1043, 9855, 6299]\n",
      "to_node: 9855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is the way that it is done by the other person\n",
    "# this results in a test accuracy of around 0.96\n",
    "min_freq = 2\n",
    "edge_stat = torch.zeros(vocab_count, vocab_count)\n",
    "\n",
    "z = 0\n",
    "for node_set, neighbor_set in zip(node_sets, neighbor_sets):\n",
    "    for neighbor in neighbor_set:\n",
    "        for to_node in neighbor:\n",
    "            if z < 2:\n",
    "                print(f'neighbor_set: {neighbor_set}')\n",
    "                print(f'neighbor: {neighbor}')\n",
    "                print(f'node_set: {node_set}')\n",
    "                print(f'to_node: {to_node}')\n",
    "                print()\n",
    "            z+=1\n",
    "            edge_stat[node_set, to_node] += 1\n",
    "public_edge_mask = edge_stat < min_freq\n",
    "#edge_df = pd.DataFrame(edge_stat.numpy())\n",
    "#edge_df.iloc[[11215, 14384, 10347, 2464, 2370],[11215, 14384, 10347, 2464, 2370]]\n",
    "#[itos[wrd] for wrd in [11215, 14384, 10347, 2464, 2370]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17370b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbor_set: [[1043, 9855, 11196], [1043, 9855, 11196, 1675], [1043, 9855, 11196, 1675, 767], [9855, 11196, 1675, 767, 3160], [11196, 1675, 767, 3160, 4042], [1675, 767, 3160, 4042, 1043], [767, 3160, 4042, 1043, 9855], [3160, 4042, 1043, 9855, 6299], [4042, 1043, 9855, 6299], [1043, 9855, 6299]]\n",
      "neighbor: [1043, 9855, 11196]\n",
      "to_node: 1043\n",
      "\n",
      "neighbor_set: [[1043, 9855, 11196], [1043, 9855, 11196, 1675], [1043, 9855, 11196, 1675, 767], [9855, 11196, 1675, 767, 3160], [11196, 1675, 767, 3160, 4042], [1675, 767, 3160, 4042, 1043], [767, 3160, 4042, 1043, 9855], [3160, 4042, 1043, 9855, 6299], [4042, 1043, 9855, 6299], [1043, 9855, 6299]]\n",
      "neighbor: [1043, 9855, 11196]\n",
      "to_node: 9855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is the way that I think that it should be done, based off of the research paper\n",
    "# where each node is connected to its neighbors, also it is bidirectional\n",
    "# however, this method results in a test accuracy around 0.90\n",
    "min_freq = 2\n",
    "edge_stat = torch.zeros(vocab_count, vocab_count)\n",
    "\n",
    "z=0\n",
    "for neighbor_set in neighbor_sets:\n",
    "    for neighbor in neighbor_set:\n",
    "        for to_node in neighbor:\n",
    "            if z < 2:\n",
    "                print(f'neighbor_set: {neighbor_set}')\n",
    "                print(f'neighbor: {neighbor}')\n",
    "                print(f'to_node: {to_node}')\n",
    "                print()\n",
    "            z+=1\n",
    "            # create neighbors_temp which removes one single occurrence of to_node,\n",
    "            # that way the edges only connect the same node with itself if it shows up multiple times in neighbor\n",
    "            neighbors_temp = copy.deepcopy(neighbor)\n",
    "            neighbors_temp.remove(to_node)\n",
    "            # connect all neighbors to the node\n",
    "            edge_stat[neighbors_temp, to_node] += 1\n",
    "            # connect all nodes to the neighbors\n",
    "            edge_stat[to_node, neighbors_temp] += 1\n",
    "public_edge_mask = edge_stat < min_freq # mark True at uncommon edges\n",
    "#edge_df = pd.DataFrame(edge_stat.numpy())\n",
    "#edge_df.iloc[[11215, 14384, 10347, 2464, 2370],[11215, 14384, 10347, 2464, 2370]]\n",
    "#[itos[wrd] for wrd in [11215, 14384, 10347, 2464, 2370]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145ad90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
